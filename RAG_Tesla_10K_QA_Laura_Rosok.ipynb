{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYvRllzDN6iQ"
      },
      "source": [
        "#Leveraging RAG for Context-Aware Analysis of Tesla’s 10-K Filing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6EAKI_5JVVT"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "# !pip install faiss-cpu\n",
        "# !pip install -U sec-edgar-downloader\n",
        "# !pip install python-dotenv\n",
        "# !pip install tqdm\n",
        "# !gcloud projects list\n",
        "# !pip install sentence-transformers\n",
        "\n",
        "from sec_edgar_downloader import Downloader\n",
        "import os\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "import textwrap\n",
        "#from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Acquisition"
      ],
      "metadata": {
        "id": "Rbp6Yr7Q_TtM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy8V-mEPL0Cq"
      },
      "outputs": [],
      "source": [
        "# initialize\n",
        "dl = Downloader('University of Illinois', 'laura.rosok@example.com')\n",
        "\n",
        "# download Tesla’s latest 10-K filings\n",
        "dl.get(\"10-K\",\"TSLA\",limit=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKbR2kWfeOiA"
      },
      "source": [
        "##Text Preprocessing and RAG Model Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOOKb-2PQZXv"
      },
      "outputs": [],
      "source": [
        "# Define the directory where filings are stored\n",
        "download_folder = 'sec-edgar-filings/TSLA/10-K/'\n",
        "\n",
        "filing_content = []\n",
        "# Walk through all files and directories in the download folder\n",
        "for root, dirs, files in os.walk(download_folder):\n",
        "    for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            # Open the file and read its content\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                filing_content.append(f.read())  # Read the entire content\n",
        "filing_content = ''.join(filing_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_filing_content(content):\n",
        "    # Remove SEC tags (e.g., <SEC-DOCUMENT>, <SEC-HEADER>, etc.)\n",
        "    content = re.sub(r'<.*?>', '', content)  # Remove XML tags\n",
        "\n",
        "    # Remove specific metadata fields (dates, accession numbers, etc.)\n",
        "    content = re.sub(r'ACCESSION NUMBER:.*?(\\n|$)', '', content)  # Remove accession number and related info\n",
        "    content = re.sub(r'CONFORMED SUBMISSION TYPE:.*?(\\n|$)', '', content)\n",
        "    content = re.sub(r'PUBLIC DOCUMENT COUNT:.*?(\\n|$)', '', content)\n",
        "    content = re.sub(r'FORM TYPE:.*?(\\n|$)', '', content)\n",
        "    content = re.sub(r'FILM NUMBER:.*?(\\n|$)', '', content)\n",
        "    content = re.sub(r'BUSINESS ADDRESS:.*?(\\n|$)', '', content)\n",
        "    content = re.sub(r'MAIL ADDRESS:.*?(\\n|$)', '', content)\n",
        "\n",
        "    # Remove other irrelevant or unwanted data patterns\n",
        "    content = re.sub(r'\\s+', ' ', content)  # Normalize excessive whitespace and newlines to single space\n",
        "    content = content.strip()  # Remove leading and trailing whitespace\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "rKfKhmshI-lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-GqoJkpTjhE"
      },
      "outputs": [],
      "source": [
        "def split_filing_into_chunks(content, chunk_size=1000):\n",
        "    # Split content into chunks of roughly `chunk_size` characters\n",
        "    chunks = []\n",
        "    for i in range(0, len(content), chunk_size):\n",
        "        chunks.append(content[i:i+chunk_size])\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhr-FimJU1n0"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Sentence-BERT model\n",
        "model = SentenceTransformer('all-mpnet-base-v2') # or 'paraphrase-MiniLM-L6-v2'\n",
        "\n",
        "# Function to embed text (chunks) into numerical vectors\n",
        "def embed_text(texts):\n",
        "    embeddings = model.encode(texts)\n",
        "    return embeddings\n",
        "\n",
        "# Create FAISS index to store and retrieve embeddings\n",
        "def create_faiss_index(chunks):\n",
        "    embeddings = embed_text(chunks)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance for the dimension of the embeddings\n",
        "\n",
        "    # Add embeddings to the index\n",
        "    index.add(np.array(embeddings).astype(np.float32))\n",
        "\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzN3xt8tUh-h"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clean the filing content\n",
        "cleaned_content = clean_filing_content(filing_content)\n",
        "\n",
        "# Step 2: Split into chunks\n",
        "chunks = split_filing_into_chunks(cleaned_content)\n",
        "\n",
        "# Step 3: Embed the chunks into vectors\n",
        "embeddings = embed_text(chunks)\n",
        "\n",
        "# Step 4: Create FAISS index\n",
        "index = create_faiss_index(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhC_4za7UfqR"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_chunks(query, index, top_k=5):\n",
        "    query_embedding = embed_text([query])  # Convert query to embedding\n",
        "    distances, indices = index.search(query_embedding.astype(np.float32), top_k)\n",
        "\n",
        "    # Collect relevant chunks, but now prioritize chunks that might contain the answer\n",
        "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
        "\n",
        "    # Concatenate top relevant chunks to give a better context to the model\n",
        "    context = '\\n'.join(relevant_chunks)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Integration"
      ],
      "metadata": {
        "id": "uajcH0-UnXpj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0ECqfUzwrA4"
      },
      "outputs": [],
      "source": [
        "# #Create an .env file in your current directory\n",
        "# with open('.env', 'w') as f:\n",
        "#     f.write('API_KEY=AIzaSyBjX-MHLX0xEcgl8NwzQB30WMxUV_8LLIQ')\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "API_KEY = os.getenv('API_KEY')\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"API_KEY not found in .env file.\")\n",
        "\n",
        "PROJECT_ID = 'rag-tesla-10k-qa'\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
        "\n",
        "gemini_model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
        "\n",
        "def generate_with_gemini(query, context):\n",
        "    max_context_length = 2048\n",
        "    truncated_context = context[:max_context_length]\n",
        "\n",
        "    prompt = f\"Context:\\n{truncated_context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    # Generate content using the Gemini model\n",
        "    response = gemini_model.generate_content([Part.from_text(prompt)])\n",
        "\n",
        "    # Return the generated text\n",
        "    # Check if candidates list is not empty and access the text\n",
        "    if response.candidates:\n",
        "        return response.candidates[0].content.parts[0].text\n",
        "    else:\n",
        "        return \"No response generated.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output"
      ],
      "metadata": {
        "id": "dwTSpAZFn0Vf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ervujqT0zPpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ab0ad1-a113-4845-a3f7-69b2da5ae16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer to '\u001b[1mWhat are Tesla's major expenses?\u001b[0m':\n",
            "\n",
            "Based on the provided text excerpt, Tesla's major expenses include:  *\n",
            "\u001b[1mAcquisitions of property and equipment:\u001b[0m  Amounts shown are $2,148,\n",
            "$2,251, and $1,088 (likely across different reporting periods). * \u001b[1mInterest\n",
            "payments (net of capitalized amounts):\u001b[0m $152, $266, and $444. * \u001b[1mTaxes\n",
            "(net of refunds):\u001b[0m $1,203, $561, and $115.   The excerpt is incomplete, and\n",
            "other expenses undoubtedly exist, but these are the only ones explicitly\n",
            "detailed in this snippet.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Answer to '\u001b[1mWhat is Tesla's approach to sustainability?\u001b[0m':\n",
            "\n",
            "Tesla's approach to sustainability is multifaceted and encompasses their entire\n",
            "energy and transportation ecosystem.  They aim to accelerate the world's\n",
            "transition to sustainable energy by addressing both energy generation and\n",
            "consumption.  This includes:  * \u001b[1mDesigning and manufacturing electric\n",
            "vehicles:\u001b[0m  Their core business is producing electric vehicles to reduce\n",
            "carbon emissions from transportation. * \u001b[1mBuilding sustainable factories:\u001b[0m\n",
            "Each new factory is designed to be more efficient and sustainable than the\n",
            "previous one, focusing on reducing waste (including per-unit waste reduction),\n",
            "and resource consumption (water and energy). * \u001b[1mDeveloping a complete energy\n",
            "system:\u001b[0m  They are working on a complete energy and transportation ecosystem,\n",
            "implying a broader approach beyond just vehicle production.  This likely\n",
            "includes their energy generation and storage solutions (solar panels,\n",
            "Powerwall). * \u001b[1mA commitment to protecting their patents:\u001b[0m  While\n",
            "aggressively protecting their intellectual property, they pledge not to sue\n",
            "parties acting in good faith in the development of electric vehicles or related\n",
            "equipment, fostering collaboration within the industry to accelerate the\n",
            "transition to sustainable energy.  In short, Tesla's sustainability strategy is\n",
            "not limited to its vehicles, but extends to the entire lifecycle of their\n",
            "products and their broader impact on the energy sector, aiming for a systemic\n",
            "change towards sustainability.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Answer to '\u001b[1mHow does Tesla manage its supply chain?\u001b[0m':\n",
            "\n",
            "Based on the provided text, Tesla's supply chain management faces challenges\n",
            "related to scaling its operations to meet rapidly increasing sales and delivery\n",
            "targets.  The text highlights concerns about:  * \u001b[1mService capacity:\u001b[0m\n",
            "Potential delays in adding servicing capacity and efficiently servicing\n",
            "vehicles, especially high-volume models like Model 3 and Model Y, could\n",
            "overburden their resources and parts inventory.  * \u001b[1mVehicle reliability:\u001b[0m\n",
            "Unforeseen issues with vehicle reliability could further strain service\n",
            "capabilities.  * \u001b[1mSupercharger infrastructure:\u001b[0m  The increasing number of\n",
            "Tesla vehicles necessitates a rapid expansion of Supercharger stations and\n",
            "connectors globally.  * \u001b[1mOverall growth:\u001b[0m There's no guarantee that Tesla\n",
            "can ramp up its business sufficiently to meet sales, delivery, installation,\n",
            "servicing, and charging targets.  This includes uncertainty about the accuracy\n",
            "of their projections and whether the pace of growth in customer infrastructure\n",
            "will meet expectations.  The text emphasizes the significant cash investments\n",
            "and management resources required for these expansion plans, with no assurance\n",
            "of a guaranteed return.  It doesn't detail *how* Tesla *manages* its supply\n",
            "chain (e.g., specific strategies, technologies, or partnerships), but rather\n",
            "focuses on the significant risks and challenges inherent in managing such a\n",
            "rapidly growing and globally dispersed operation.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample queries\n",
        "queries = [\n",
        "    \"What are Tesla's major expenses?\",\n",
        "    \"What is Tesla's approach to sustainability?\",\n",
        "    \"How does Tesla manage its supply chain?\"\n",
        "]\n",
        "\n",
        "# Function to format response\n",
        "def bold_response(response):\n",
        "    # Replace '**word**' with markdown bold formatting\n",
        "    response = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\033[1m\\1\\033[0m', response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Retrieve and combine relevant chunks for each query\n",
        "for query in queries:\n",
        "    relevant_chunks = retrieve_relevant_chunks(query, index)\n",
        "    context = '\\n'.join(relevant_chunks)\n",
        "\n",
        "    # Get the answer from Gemini\n",
        "    response = generate_with_gemini(query, context)\n",
        "\n",
        "    # Format the response as per the requested rules\n",
        "    formatted_response = bold_response(response).strip()\n",
        "\n",
        "    # Wrap response text to a fixed width\n",
        "    wrapped_response = textwrap.fill(formatted_response, width=80)\n",
        "\n",
        "    # Print formatted response\n",
        "    print(f\"Answer to '\\033[1m{query}\\033[0m':\\n\")\n",
        "    print(wrapped_response)\n",
        "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}